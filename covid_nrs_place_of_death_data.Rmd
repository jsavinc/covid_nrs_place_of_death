---
title: "Place of death in NRS COVID-19 deaths: data processing for analysis"
author: "Jan Savinc"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Load packages

```{r, warning=FALSE, message=FALSE}
## install packages from github if not yet available!
# remotes::install_github("datasciencescotland/opendatascot", force = TRUE)
# remotes::install_github("Health-SocialCare-Scotland/phsmethods", force = TRUE)

library(tidyverse)  # for tidy workflow
library(opendatascot)  # importing data from ScotGov open data website
library(phsmethods)  # methods for working with PHS data
library(readxl)  # for reading excel files
# library(SPARQL)  # taken care of by opendatascot
library(lubridate)  # dealing with dates
library(janitor)  # for cleaning column names
library(ISOweek)  # for computing date from ISO week number + year
library(sf)  # for mapping
library(ggrepel)  # for 'mapping'repelling' labels/texst in ggplot
library(patchwork)  # for assembling plots
library(extrafont)  # for working with fonts
library(openxlsx)  # for creating xlsx files
library(knitr)  # for displaying tables in rmd
```


# Set up SPARQL query

Note: not needed if using `opendatascot`
To access the latest NRS data using their API, we first setup a SPARQL query.

```{r}
# endpoint <- "http://statistics.gov.scot/sparql.json"
endpoint <- "http://statistics.gov.scot/sparql"
```


# Copyright attribution

All data used for this study were obtained from the National Records Scotland (NRS) and are Â© Crown copyright, 2020; the details of the licence can be viewed on the [Open Government Licence website](http://www.nationalarchives.gov.uk/doc/open-government-licence/open-government-licence.htm)


# Dates represent event registration, not occurrence!

From NRS website:

> All routine vital events information we publish is based on the date of registration, not the date on which the event occurred. Inevitably, of course, there are delays between the occurrence of an event and its registration.


# Context of lockdowns

A recent timeline of COVID-19 events in Scotland is [available here](https://spice-spotlight.scot/2021/01/29/timeline-of-coronavirus-covid-19-in-scotland/). Including these might help in the interpreration fo the figures, though the figures are already pretty busy. This is a potential future TODO!


# Load info about latest data

A separate script checks for updated data and downloads it, also updating these files with the locations of the downloaded data:

```{r}
file_data_sources <- "./table_of_data_sources.csv"
file_historical_data_sources <- "./table_of_historical_data_sources.csv"
file_case_trends_data_sources <- "./table_of_case_trends_data_sources.csv"

table_of_data_sources <- read_csv(file = file_data_sources)
table_of_historical_data_sources <- read_csv(file = file_historical_data_sources)
table_of_case_trends_data_sources <- read_csv(file = file_case_trends_data_sources)

table_of_data_sources %>%
  knitr::kable(caption = "Latest data used in this report.")
```


## Set up output directory

Because data are updated on a weekly basis, we'll have a separate directory for every week the data are updated:

```{r}
latest_data_modified_date <- strftime(max(table_of_data_sources$last_modified), format="%F")

dir_outputs <- paste0("./outputs/",latest_data_modified_date)
if (!dir.exists(dir_outputs)) dir.create(dir_outputs, recursive = TRUE)
```


# Import & wrangle data

We're importing two kinds of data:

1. Weekly deaths involving COVID-19 from Scottish gov't open data service, broken down by week & place of death (home or institution)
2. Historical deaths 2015-2019, grouped by week & place of death

For both of these, we'll also look at crosstabulated figures for:

* Health board
* Local Authority
* Sex
* Age
* ~~Ethnicity~~ *Note: not sure this is available*

There are two ways of obtaining the data:

1. ~~Using their SPARQL-based API ("data cube"), which can be accessed using the `opendatascot` package in R; the data can be seen on [the Scottish Gov't open data website](https://statistics.gov.scot/data/deaths-involving-coronavirus-covid-19)~~
    - Update: it's difficult to keep track of data updates using the API - it's simpler to query the .xlsx/.csv files (next bullet point) for when they were last modified and download the weekly update once it's been released!
2. There are a number of datasets, including the demographic/geographic crosstabulations available on the "Related Statistics" section of the [deaths involving COVID-19 website]( https://www.nrscotland.gov.uk/statistics-and-data/statistics/statistics-by-theme/vital-events/general-publications/weekly-and-monthly-data-on-births-and-deaths/deaths-involving-coronavirus-covid-19-in-scotland), under the *Ad-hoc queries section*. They are updated weekly, and also contain historical figures.

Note that the [*About* tab of the "data cube" interface](https://statistics.gov.scot/resource?uri=http%3A%2F%2Fstatistics.gov.scot%2Fdata%2Fdeaths-involving-coronavirus-covid-19) says the following about Health Board & Local Authority figures:

>Health Board and Local Authority figures include non-residents. Deaths are allocated to areas based on the usual residence of the deceased. If the deceased was not a Scottish resident, the death is allocated to the area where the death occurred.

This means we'll need to download those figures from the website rather than using the API!

Note that the weekly deaths due to COVID-19 in 2020 & 2021 also include a weekly average 2015-2019, but for the purposes of showing the range of deaths (weekly minima & maxima), we need the weekly figures for individual years & the appropriate cross-tabulations.

```{r}
# TODO: remove this section, params not used anymore
# # "unpack" the data source tables
# table_of_data_sources <- params$data_sources
# table_of_historical_data_sources <- params$historical_data_sources
# table_of_case_trends_data_sources <- params$case_trends_data_sources
```


## Parsing week numbers

In the `opendatascot`-imported data, `refPeriod` refers to the week number (called `week of occurrence` in the `.xlsx`/`.csv`-imported data has), as defined by [ISO 8601](https://en.wikipedia.org/wiki/ISO_week_date), and can be computed from a date with `lubridate::isoweek()`; the first week of 2020 is the week starting with Monday 2019-12-30, ~~and the corresponding start dates for each week can be computed by adding weeks using `lubridate::dweeks()` to that date.~~ This is non-trivial, because the number of iso weeks in a year isn't constant. The `ISOweek::ISOweek2date()` function deals with this issue in a crude manner, and allows us to compute the date (w/c on date) from the year & week number. It relies on being provided a valid week number (some years have 52 and some have 53), but it's probably safe to assume that the NRS data has valid dates.

Invalid week numbers will produce a date in the next year: e.g. week 53 in 2021 would actually start in 2022, which makes it invalid. We can test for that by checking that the year provided is the same year in the date produced.

We'll also compute a 'run-over' week number of 2021 in case we plot both years on a contibuous X-axis - in this case, week 1 of 2021 follows week 53 of 2020, so we start counting from 54 in 2021.

```{r}
date_first_week_2020 <- ymd("2019-12-30")
ISOweek::ISOweek2date(weekdate = "2019-W53-1")
ISOweek::ISOweek2date(weekdate = "2020-W01-1")
ISOweek::ISOweek2date(weekdate = "2020-W52-1")
ISOweek::ISOweek2date(weekdate = "2020-W53-1")
ISOweek::ISOweek2date(weekdate = "2021-W01-1")
ISOweek::ISOweek2date(weekdate = "2021-W53-1")

compute_start_date_from_week_number <- function(week_number, year_number) {
  ## this assumes a valid week number! There are 52 or 53 weeks in a year
  isoweek_string <- glue::glue("{year_number}-W{str_pad(week_number, width=2, pad='0')}-1")
  # computed_dates <- ISOweek::ISOweek2date(isoweek_string)
  computed_dates <- rep(NA_Date_,times = length(week_number))
  computed_dates[which(!is.na(week_number))] <- ISOweek::ISOweek2date(isoweek_string[which(!is.na(week_number))])  # return NA instead of failing with ISOweek!
  ## invalid weeks will result in a computed date in a year after the requested year...
  ## a valid week can result in a date that starts in the year before!
  if (any(year_number<year(ymd(computed_dates)), na.rm = TRUE)) stop("invalid week number provided!")
  return(computed_dates)
}

tibble(
    week_number = c(1,2,3,52,53)
  ) %>%
  mutate(
    date = compute_start_date_from_week_number(week_number = week_number, year_number = 2020),
    iso_week_from_date = isoweek(date)
  ) %>% knitr::kable(caption = "Examples of computing start date for each week, and computing isoweek from the dates!")
```


## Consistent Place of death coding

Place of death was named differently between datasets (location/place) and the different places were also named slightly differently, so we find all of them and recode them to the same:

```{r}
order_of_place_of_death_levels <- c("Hospital", "Care home", "Home & other non-institution", "Other", "All")

recode_place_of_death <- function(data_tbl) {
  data_tbl %>%
    mutate(place_of_death = case_when(
      str_detect(place_of_death, pattern = regex(pattern = "all", ignore_case = TRUE)) ~ "All",
      str_detect(place_of_death, pattern = regex(pattern = "care.*home", ignore_case = TRUE)) ~ "Care home",
      str_detect(place_of_death, pattern = regex(pattern = "(non.*institut)|home", ignore_case = TRUE)) ~ "Home & other non-institution",
      str_detect(place_of_death, pattern = regex(pattern = "hospital", ignore_case = TRUE)) ~ "Hospital",
      str_detect(place_of_death, pattern = regex(pattern = "other", ignore_case = TRUE)) ~ "Other",
      TRUE ~ NA_character_
      ) %>%
        factor(x = ., levels = order_of_place_of_death_levels)
    )
}
```


## Consistent Cause of death coding

The `opendatascot`-imported data distinguishes "all causes" deaths from "covid-related" deaths - the non-covid-related deaths are the difference between those. In contrast, the other weekly datasets code cause of death as either covid-related or non-covid-related, in which case we need to compute all cause deaths as the sum of the two.

We'll change the wording of these to a consistent format so it's easier to compute consistent columns. The below function will also compute non-covid deaths for data where all deaths and covid deaths are provided, and all deaths where covid- and non-covid-related deaths are provided.

This involves pivotting the number of deaths variable into wide format based on the cause of death. Because of this, we need to deal with implicit missing values:

The first COVID-19-related deaths were reported from week 12 onwards. In some of the datasets, a missing/omitted value represents 0 cases in that category, when a 0 should have been recorded. E.g. deaths in "Other institutions" are very low, and if a record is missing for that category, we should assume 0 deaths.

On 3 Februrary 2021 as I was updating the script, the weekly figures started distinguishing cause of death: COVID-19 can be a contributory factor, or an underlying cause of death. For the purposes of this study, COVID-19 deaths are in the minority, and we will disregard this distinction for now.


```{r}
## helper function to make consistently coded cause of death
recode_cause_of_death <- function(data_tbl) {
  data_tbl %>%
    mutate(
      cause_of_death = case_when(
        cause_of_death == "all-causes" ~ "deaths_all_causes",
        cause_of_death == "all-causes-average-of-corresponding-week-over-previous-5-years" ~ "deaths_nrs_past_average_all_causes",
        cause_of_death == "covid-19-related" ~ "deaths_covid_related",
        cause_of_death == "COVID-19 mentioned" ~ "deaths_covid_related",
        cause_of_death == "COVID-19 contributory factor" ~ "deaths_covid_related",
        cause_of_death == "COVID-19 underlying cause" ~ "deaths_covid_related",
        cause_of_death == "Non-COVID-19" ~ "deaths_non_covid",
        TRUE ~ NA_character_
      )
    )
}

# TODO:  still too complicated to safely use as a function...
## find which combinations of geographic area, cause of death, and place of death are missing and set the number of deaths to 0 for them
# complete_weekly_deaths_with_implicit_zeros <- function(data_tbl) {
#   data_tbl %>%
#   tidyr::complete(
#       nesting(year, week_number, week_number_run_over, date_w_c),  # these are 'identifying' variables that shouldn't be combinatorily exploded
#       ref_area, cause_of_death, place_of_death,  # these are the variables we want to have all combinations of, and missing combinations are set to 0 deaths
#       fill = list(number_of_deaths = 0)
#       )
# }

## calculate number of deaths after recoding cause of death -
## recoding results in two entries for covid-related deaths, so we need to sum over them
## this is achieved by grouping over all variables other than number of deaths
recalculate_number_of_deaths_after_recoding_cause_of_death <- function(data_tbl) {
  data_tbl %>%
  group_by_all %>% 
  ungroup(number_of_deaths) %>%  # this is done by grouping over everything except the number of deaths
  summarise(number_of_deaths = sum(number_of_deaths), .groups = "drop")  # recompute the sum of deaths
}

## covid death reporting started in week 12, 2020 - before then it doesn't make sense to distunguish between covid and non-covid deaths, and only the all cause death figure should be kept
## by using the 'overrun' weeks we ensure that 2021 or later isn't affected
## note: we're keeping all cause deaths!
set_covid_and_non_covid_deaths_to_zero_before_reporting_started <- function(data_tbl) {
  data_tbl %>%
    mutate(
      deaths_non_covid = if_else(
        condition = week_number_run_over < 12,
        true = NA_real_,
        false = deaths_non_covid
      ),
      deaths_covid_related = if_else(
        condition = week_number_run_over < 12,
        true = NA_real_,
        false = deaths_covid_related
      )
    )
}

# TODO: delete below function, it's too complicated and I've broken it down to the individual functions
# recode_cause_of_death_and_compute_columns <- function(data_tbl) {
#   potential_grouping_variables <- c("ref_area","sex","age","ref_period")
#   grouping_variables <- potential_grouping_variables[which(potential_grouping_variables %in% names(data_tbl))]
#   
#   interim_data_tbl <- 
#     data_tbl %>%
#     recode_cause_of_death() %>%
#     tidyr::complete(
#       nesting(year, week_number, week_number_run_over, date_w_c),
#       !!!rlang::syms(grouping_variables),
#       cause_of_death, place_of_death,
#       fill = list(number_of_deaths = 0)
#       ) %>%
#     pivot_wider(names_from = "cause_of_death", values_from = "number_of_deaths")
#   
#   ## compute non-covid deaths from all deaths minus covid deaths
#   if (!"deaths_non_covid" %in% names(interim_data_tbl)) {
#     interim_data_tbl <-
#       interim_data_tbl %>%
#       mutate(deaths_non_covid = deaths_all_causes - deaths_covid_related)
#   }
#   ## compute all deaths from non-covid deaths + covid deaths
#   if (!"deaths_all_causes" %in% names(interim_data_tbl)) {
#     interim_data_tbl <-
#       interim_data_tbl %>%
#       mutate(deaths_all_causes = deaths_non_covid + deaths_covid_related)
#   }
#   
#   interim_data_tbl <-
#     interim_data_tbl %>%  # set as missing covid- & non-covid-related deaths before week 12
#     mutate(
#       deaths_non_covid = if_else(
#         condition = week_number_run_over < 12,
#         true = NA_real_,
#         false = deaths_non_covid
#       ),
#       deaths_covid_related = if_else(
#         condition = week_number_run_over < 12,
#         true = NA_real_,
#         false = deaths_covid_related
#       )
#     )
#   
#   return(interim_data_tbl)
# }
```


## Consistent use of missing value

```{r}
replace_na_with_0_from_week_12 <- function(data_tbl) {
  data_tbl %>%
    mutate(
      value = if_else(
        condition = week_number_roll_over >=12 & is.na(number_of_deaths),
        true = 0,
        false = number_of_deaths
      )
    )
}
```


## Merging health boards

Smaller health boards were merged... TODO!

> "Note: Orkney, Shetland, & Western health boards were merged into 'Island HBs'; Borders & Dumfries and Galloway health boards were merged into 'B,D&G HB'"


## Load weekly COVID-19 related deaths

~~Using `opendatascot` to load weekly deaths the level of geography can be specified, so we can have more or less granular data~~ Choosing a more granular level than Scotland-wide data means place of death is collapsed to the 'all' category only, see note above! Even for the Scotland-wide data, place of death is collapsed to 'All' for the granular sex & age categories, actually.

I've also attempted loading the case counts using `ckanr` using the CKAN API from the opendata.nhs.scot website, but gave that up in favour of using their `.csv` links instead to download daily-updated data.

Note about weekly LA/HB data:

> From 14/12/2020 confirmed positive cases are now allocated to an area based on the postcode recorded in the testing system. Previously cases were allocated to local areas mainly based on their usual residential address recorded in the Community Health Index (CHI) database. This change will impact on newly reported cases on this day.

> Please note that the updated methodology for determining COVID-19 related ICU admissions has been applied to the historic data. The updated definition is as follows: A patient who has tested positive for COVID at any time in the 21 days prior to admission to ICU, or who have tested positive from the date of admission up to and including the date of ICU discharge.

> In addition, on 30th October it came to our attention that there is an ongoing issue when linking ICU data to the lab data for COVID-19 test results. Any COVID-19 positive patients with a missing CHI that had a first positive test in the community are unable to be linked to the ICU data. As a result the COVID-19 positive numbers could be underreported by a small number.


`crude_rate_positive`: Crude rate of total positive cases (cumulative) per 100,000 population.

### Define locations for weekly data fiels

```{r}
download_dir <- "./downloaded_data"
if (!dir.exists(download_dir)) dir.create(download_dir)  # create download dir if it doesn't exist

files_weekly_deaths <- set_names(x = as.list(table_of_data_sources$path_latest), nm = table_of_data_sources$short_name)
```

### Load  'main report' data

We are only loading deaths aggregated by cause from the 'main report' data! These are compiled in two sheets:

* Table 3 (2020)
* Table 3  (2021)

However, the number of spaces might differ! So we will extract the table names automatically.

The ICD-10 codes used for causes of death:

>4) The ICD 10 codes for disease categories are as follows:	
Cancer: C00-C97	
Dementia and Alzheimer's: F01, F03 and G30	
Circulatory: I00:I99	
Respiratory: J00-J99	
COVID-19: U07	

The format is a bit complicated to read automatically; we will extract the number of weeks available in the data from the filename.


~~These data come chopped up into 4 sheets, 2 by 2 for each of year 2020/2021 and all deaths/covid-related deaths. They are named:

* Table 1 (2020)
* Table 1 (2021)
* Table 2 (2020)
* Table 2 (2021)

~~


```{r}
weeks_available_in_nrs_weekly_data_by_cause_of_death_2021 <-  # extract the maximum weeks available from the weekly-updated 2021 data
  files_weekly_deaths$main_report %>%
  str_extract(string = ., pattern = "week-\\d{2}") %>%  # extract eg "week-12"
  str_extract(string = ., pattern = "\\d{2}") %>%  # extract "12"
  as.integer()

## extract the sheet names for cause of death data
nrs_weekly_data_sheet_2020 <- 
  excel_sheets(files_weekly_deaths$main_report)[which(str_detect(tolower(excel_sheets(files_weekly_deaths$main_report)), pattern = "table 3.*2020"))]
nrs_weekly_data_sheet_2021 <- 
  excel_sheets(files_weekly_deaths$main_report)[which(str_detect(tolower(excel_sheets(files_weekly_deaths$main_report)), pattern = "table 3.*2021"))]

## construct column names for the data
headers_in_nrs_data_2020 <-
  c(
    "place_of_death",
    "cause_of_death",
    paste0("week_", 1:53),
    "blank_remove",  # this and the rest are irrelevant entries that we'll remove
    "remove_1",
    "remove_2"
  )
headers_in_nrs_data_2021 <-
  c(
    "place_of_death",
    "cause_of_death",
    paste0(
      "week_",
      1:weeks_available_in_nrs_weekly_data_by_cause_of_death_2021
    ),
    "blank_remove",  # this and the rest are irrelevant entries that we'll remove
    "remove_1"
  )
## in week 32 of 2021, the format changed somewhat and the blank column in penultimate location in Table 3 (2021) was removed
## so this is a hack that gets around that!
expected_ncol_2021 <- length(headers_in_nrs_data_2021)
actual_ncol_2021 <- ncol(readxl::read_excel(files_weekly_deaths$main_report, sheet = nrs_weekly_data_sheet_2021))
headers_in_nrs_data_2021 <- headers_in_nrs_data_2021[1:actual_ncol_2021]  # subset the first N of the defined headers, where N is actual number of columns in file

## extract data... this is dense but I've tried to annotate it clearly!
weekly_deaths_cause_of_death_before_split_2020 <-
  readxl::read_excel(files_weekly_deaths$main_report, sheet = nrs_weekly_data_sheet_2020, col_names = headers_in_nrs_data_2020, range = cell_rows(6:125)) %>%  # the data are inrows 6:125
  select(-matches("remove")) %>%  # remove irrelevant summary columns at the end
  mutate(across(.cols = matches("week"), .fns = as.numeric)) %>%  # remove erroneous "count1, count2, ..." entries due to superscript in previous column
  mutate(dataset = if_else(condition = str_detect(string = cause_of_death, pattern = regex(pattern = "^registered|^difference", ignore_case = TRUE)), true = cause_of_death, false = NA_character_)) %>%  # extract the Registered deaths: historical, 2020/2021, and Difference sections
  tidyr::fill(place_of_death, .direction = "down") %>%  # complete the entries for place of death from the "last observed"
  tidyr::fill(dataset, .direction = "down") %>%  # same as above but for "dataset" column
  drop_na(week_1) %>%  # remove blank "header" rows with no data recorded
  pivot_longer(cols = matches("week"), names_to = "week_number", values_to = "number_of_deaths", values_drop_na = TRUE) %>%  # convert to long data format
  mutate(
    week_number = as.integer(parse_number(week_number)),  # extract week number
    place_of_death =  # find best match in the reference names used in other datasets
      order_of_place_of_death_levels[stringdist::amatch(x = str_sub(place_of_death,1,4), table = str_sub(order_of_place_of_death_levels,1,4), method = "lv", maxDist = 5)]  # the reference is in the variable order_of_place_of_death_levels
    )

weekly_deaths_cause_of_death_before_split_2021 <-
  readxl::read_excel(files_weekly_deaths$main_report, sheet = nrs_weekly_data_sheet_2021, col_names = headers_in_nrs_data_2021, range = cell_rows(6:125)) %>%  # the data are inrows 6:125
  select(-matches("remove")) %>%  # remove irrelevant summary columns at the end
  mutate(across(.cols = matches("week"), .fns = as.numeric)) %>%  # remove erroneous "count1, count2, ..." entries due to superscript in previous column
  mutate(dataset = if_else(condition = str_detect(string = cause_of_death, pattern = regex(pattern = "^registered|^difference", ignore_case = TRUE)), true = cause_of_death, false = NA_character_)) %>%  # extract the Registered deaths: historical, 2020/2021, and Difference sections
  tidyr::fill(place_of_death, .direction = "down") %>%  # complete the entries for place of death from the "last observed"
  tidyr::fill(dataset, .direction = "down") %>%  # same as above but for "dataset" column
  drop_na(week_1) %>%  # remove blank "header" rows with no data recorded
  pivot_longer(cols = matches("week"), names_to = "week_number", values_to = "number_of_deaths", values_drop_na = TRUE) %>%  # convert to long data format
  mutate(
    week_number = as.integer(parse_number(week_number)),  # extract week number
    place_of_death =  # find best match in the reference names used in other datasets
      order_of_place_of_death_levels[stringdist::amatch(x = str_sub(place_of_death,1,4), table = str_sub(order_of_place_of_death_levels,1,4), method = "lv", maxDist = 5)]  # the reference is in the variable order_of_place_of_death_levels
    )

## compile the weekly data
weekly_deaths_cause_of_death <-
  bind_rows(
    weekly_deaths_cause_of_death_before_split_2020 %>% filter(!str_detect(tolower(dataset), pattern = "difference|average")),
    weekly_deaths_cause_of_death_before_split_2021 %>% filter(!str_detect(tolower(dataset), pattern = "difference|average"))
  ) %>%
  mutate(
    year = as.integer(parse_number(dataset)),
    date_w_c = compute_start_date_from_week_number(week_number = week_number, year = year),
    week_number_run_over = if_else(year == 2021, 53L + week_number, week_number)  # invent a "run-over" week number - weeks 54+ are in 2021
    ) %>%
  select(-dataset)

## compile historical data - note that it's already summarised as average over the 5 year period, so we don't have the raw data, nor do we have minima/maxima
historical_average_cause_of_death <-
  weekly_deaths_cause_of_death_before_split_2020 %>%  # only data from the 2020 sheet are used - the 2021 data is the same entries for historical deaths!
  filter(str_detect(tolower(dataset), pattern = "average")) %>%
  select(-dataset) %>%
  rename(deaths_mean = number_of_deaths) %>%
  mutate(deaths_min = NA_real_, deaths_max = NA_real_)
```


### Load 'ad-hoc' query data

These are data aggregated by sex, age, and region (HB/LA)!

```{r}
# checking out structure

# ods_all_datasets() %>% View()
# ods_structure("deaths")  # can we import all deaths-related data this way?
# ods_structure("coronavirus-covid-19-management-information")

# ods_dataset("coronavirus-covid-19-management-information", geography = "hb") %>%
#   filter(variable == "testing-new-cases-reported")
# 
# ods_dataset("coronavirus-covid-19-management-information", geography = "hb", variable = "testing-new-cases-reported")
# ods_dataset("coronavirus-covid-19-management-information", geography = "la", variable = "testing-new-cases-reported")
# ods_dataset("coronavirus-covid-19-management-information", variable = "testing-new-cases-reported")
# 
# ods_print_query(dataset = "coronavirus-covid-19-management-information", geography = "hb", variable = "testing-new-cases-reported") %>% dput
# ods_print_query(dataset = "coronavirus-covid-19-management-information", variable = "testing-new-cases-reported") %>% dput
#   
# modified_query <- "PREFIX qb: <http://purl.org/linked-data/cube#>                 PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>                 PREFIX xsd: <http://www.w3.org/2001/XMLSchema#> select   ?refArea ?refPeriod ?measureType ?variable ?value where { ?data qb:dataSet <http://statistics.gov.scot/data/coronavirus-covid-19-management-information>. ?data <http://purl.org/linked-data/sdmx/2009/dimension#refArea> ?refArea. ?data <http://purl.org/linked-data/sdmx/2009/dimension#refPeriod> ?refPeriod. ?data <http://purl.org/linked-data/cube#measureType> ?measureType. ?data <http://statistics.gov.scot/def/dimension/variable> ?variable. ?data ?measureType ?value.  filter (regex(str(?variable ), 'testing-new-cases-reported'))  } order by ?refPeriod ?refArea"
# 
# modified_query %>%
#   opendatascot:::ods_query_database(endpoint, query = .) %>%
#   opendatascot:::pre_process_data()
# 
# query <- ""

# 
# library(ckanr)
# ckanr_setup(url = "https://www.opendata.nhs.scot/")
# # package_list(as = "table")
# # tag_list(as = "table")
# package_search(q = 'daily', rows = 2, as = 'table') %>% str
# 
# 
# download_data_from_opendata_via_ckan_api <- function(resource_id) {
#   ckan <- ckanr::src_ckan("https://www.opendata.nhs.scot/")
#   dplyr::tbl(src = ckan$con, from = resource_id_daily_case_trends_la, ) %>% 
#   as_tibble()
# }
# 
# resource_id_daily_case_trends_la <- "427f9a25-db22-4014-a3bc-893b68243055"
# 
# resource_show(id = resource_id_daily_case_trends_la, as = "table")

# ods_print_query("deaths")

# ods_structure("deaths-involving-coronavirus-covid-19")
# ods_schemes("deaths-involving-coronavirus-covid-19")
# ods_print_query("deaths-involving-coronavirus-covid-19")
# ods_print_query("deaths-involving-coronavirus-covid-19", geography = "la")
# ods_find_lower_geographies("S92000003")  # Scotland's refArea = S92000003
# phsmethods::match_area("S92000003")  # "Scotland"

## helper function that links the 9-character geography code to its name
make_human_readable_ref_area <- function(data_tbl, ref_area = refArea) {
  enquoted_ref_area <- enquo(ref_area)
  data_tbl %>%
    mutate(
      !!enquoted_ref_area := phsmethods::match_area(!!enquoted_ref_area)
    )
}

# raw_covid_deaths <- list(
#   scotland = ods_dataset("deaths-involving-coronavirus-covid-19", geography = "sc"),
#   health_board = ods_dataset("deaths-involving-coronavirus-covid-19", geography = "hb"),
#   local_authority = ods_dataset("deaths-involving-coronavirus-covid-19",geography = "la")
# ) %>%
#   map(.x = ., .f = ~make_human_readable_ref_area(data_tbl = .x))

weekly_deaths_overall <-
  ods_dataset("deaths-involving-coronavirus-covid-19", geography = "sc") %>%
  make_human_readable_ref_area %>%
  clean_names() %>%
  ## below series of rename operations are a stupid way of dealing with the possibility of different variable name conventions in the ods_dataset data: sometimes they are in snake_case, and sometimes in camelCase, seemingly unpredictably
  rename_at(.vars = vars(matches("location", ignore.case = TRUE)), ~"place_of_death") %>%
  rename_at(.vars = vars(matches("period", ignore.case = TRUE)), ~"ref_period") %>%
  rename_at(.vars = vars(matches("area", ignore.case = TRUE)), ~"ref_area") %>%
  rename_at(.vars = vars(matches("measure", ignore.case = TRUE)), ~"measure_type") %>%
  rename_at(.vars = vars(matches("cause", ignore.case = TRUE)), ~"cause_of_death") %>%
  rename_at(.vars = vars(matches("sex", ignore.case = TRUE)), ~"sex") %>%
  rename_at(.vars = vars(matches("age", ignore.case = TRUE)), ~"age") %>%
  rename_at(.vars = vars(matches("value", ignore.case = TRUE)), ~"number_of_deaths") %>%  # rename so it's consistent with other data
  filter(!ref_period %in% as.character(2020:2021)) %>% # remove totals for 2020 & 2021, we only want the weekly figures
  mutate(
    year = parse_integer(str_extract(ref_period, pattern="^(2020|2021)")),
    week_number = parse_integer(str_remove(ref_period, pattern = "^(2020|2021)\\-")),
    date_w_c = compute_start_date_from_week_number(week_number = week_number, year = year),
    number_of_deaths = as.numeric(number_of_deaths),  # convert to numeric!
    week_number_run_over = if_else(year == 2021, 53L + week_number, week_number)  # invent a "run-over" week number - weeks 54+ are in 2021
  ) %>%
  select(-c(measure_type)) %>%  #  measure_type is count for all data
  recode_place_of_death() %>%
  recode_cause_of_death() %>%
  recalculate_number_of_deaths_after_recoding_cause_of_death %>%  # not needed here probably, but no harm in including it
  pivot_wider(names_from = cause_of_death, values_from = number_of_deaths) %>%  # this dataset is well-formatted and has no implicit missing values!
  mutate(deaths_non_covid = deaths_all_causes - deaths_covid_related)  # compute non-covid deaths from the other two figures - will be NA where either is missing


weekly_deaths_sex_age <-  # structure here is a bit more complicated - headers in row 3, but sex & age headers in row 4
  readxl::read_excel(files_weekly_deaths$sex_age, sheet = 2, skip = 4, col_names = c("week_number","place_of_death","sex","age","cause_of_death","number_of_deaths"), guess_max = 1e5) %>%
  drop_na(number_of_deaths) %>%    # remove empty rows & one row where it's just the copyright notice
  mutate(
    year = 2000L + parse_integer(str_sub(week_number, 1, 2)),
    week_number = parse_integer(str_sub(week_number, 4, 5)),
    date_w_c = compute_start_date_from_week_number(week_number = week_number, year = year),
    week_number_run_over = if_else(year == 2021, 53L + week_number, week_number)  # invent a "run-over" week number - weeks 54+ are in 2021
  ) %>%
  recode_cause_of_death() %>%
  recalculate_number_of_deaths_after_recoding_cause_of_death %>%  # recompute the sum of deaths
  tidyr::complete(  # complete implicit missing entries and make them 0
    nesting(week_number,week_number_run_over,date_w_c,year),
    age,sex,place_of_death,cause_of_death,
    fill = list(number_of_deaths = 0)
  ) %>%
  recode_place_of_death() %>%  # this needs to be done after recoding cause of death, otherwise we also include the "all" figure which isn't included in this dataset
  pivot_wider(names_from = cause_of_death, values_from = number_of_deaths) %>%   # make separate columns for causes of death figures
  mutate(deaths_all_causes = deaths_non_covid + deaths_covid_related) %>%  # compute all cause deaths from the other two figures reported
  set_covid_and_non_covid_deaths_to_zero_before_reporting_started()

# 
# weekly_deaths_la_file <-
#   curl_download(url = "https://www.nrscotland.gov.uk/files//statistics/covid19/weekly-deaths-by-date-council-area-location.xlsx",
#                 destfile = tempfile(),
#                 quiet = FALSE)

weekly_deaths_la <- 
  read_excel(files_weekly_deaths$la, sheet = 2, skip = 2) %>% 
  clean_names %>%
  rename(week_number = week_of_occurrence, ref_area = council, place_of_death = location_of_death, number_of_deaths = deaths) %>%
  filter(!is.na(number_of_deaths)) %>%  # remove blank rows & copyright entry
  mutate(
    year = 2000L + parse_integer(str_sub(week_number, 1, 2)),
    week_number = parse_integer(str_sub(week_number, 4, 5)),
    date_w_c = compute_start_date_from_week_number(week_number = week_number, year = year),
    week_number_run_over = if_else(year == 2021, 53L + week_number, week_number)  # invent a "run-over" week number - weeks 54+ are in 2021
  ) %>%
  recode_cause_of_death() %>%
  recalculate_number_of_deaths_after_recoding_cause_of_death %>%  # recompute the sum of deaths
  tidyr::complete(
      nesting(year, week_number, week_number_run_over, date_w_c),  # these are 'identifying' variables that shouldn't be combinatorily exploded
      ref_area, cause_of_death, place_of_death,  # these are the variables we want to have all combinations of, and missing combinations are set to 0 deaths
      fill = list(number_of_deaths = 0)
      ) %>%
  # complete_weekly_deaths_with_implicit_zeros() %>%
  recode_place_of_death() %>%  # this needs to be done before recoding cause of death, otherwise we also include the "all" figure which isn't included in this dataset
  pivot_wider(names_from = cause_of_death, values_from = number_of_deaths) %>%   # make separate columns for causes of death figures
  mutate(deaths_all_causes = deaths_non_covid + deaths_covid_related) %>%  # compute all cause deaths from the other two figures reported
  set_covid_and_non_covid_deaths_to_zero_before_reporting_started()


# weekly_deaths_hb_file <-
#   curl_download(url = "https://www.nrscotland.gov.uk/files//statistics/covid19/weekly-deaths-by-date-health-board-location.xlsx",
#                 destfile = tempfile(),
#                 quiet = FALSE)

weekly_deaths_hb <- 
  read_excel(files_weekly_deaths$hb, sheet = 2, skip = 2) %>% 
  clean_names %>%
  rename(week_number = week_of_occurrence, ref_area = health_board, place_of_death = location_of_death, number_of_deaths = deaths) %>%
  filter(!is.na(number_of_deaths)) %>%  # remove blank rows & copyright entry
  mutate(
    year = 2000L + parse_integer(str_sub(week_number, 1, 2)),
    week_number = parse_integer(str_sub(week_number, 4, 5)),
    date_w_c = compute_start_date_from_week_number(week_number = week_number, year = year),
    week_number_run_over = if_else(year == 2021, 53L + week_number, week_number)  # invent a "run-over" week number - weeks 54+ are in 2021
  ) %>%
  recode_cause_of_death() %>%
  recalculate_number_of_deaths_after_recoding_cause_of_death %>%  # recompute the sum of deaths
  tidyr::complete(
      nesting(year, week_number, week_number_run_over, date_w_c),  # these are 'identifying' variables that shouldn't be combinatorily exploded
      ref_area, cause_of_death, place_of_death,  # these are the variables we want to have all combinations of, and missing combinations are set to 0 deaths
      fill = list(number_of_deaths = 0)
      ) %>%
  # complete_weekly_deaths_with_implicit_zeros() %>%
  recode_place_of_death() %>%  # this needs to be done before recoding cause of death, otherwise we also include the "all" figure which isn't included in this dataset
  pivot_wider(names_from = cause_of_death, values_from = number_of_deaths) %>%  # make separate columns for causes of death figures
  mutate(deaths_all_causes = deaths_non_covid + deaths_covid_related) %>%  # compute all cause deaths from the other two figures reported
  set_covid_and_non_covid_deaths_to_zero_before_reporting_started()

## compute weekly data for sex and age separately
## sum over sex or age, respectively
weekly_deaths_sex <-
  weekly_deaths_sex_age %>%
  filter(age!="all") %>%
  group_by(week_number, week_number_run_over, year, date_w_c, sex, place_of_death) %>%
  summarise(across(starts_with("deaths_"), ~sum(.x)), .groups = "drop")

weekly_deaths_age <-
  weekly_deaths_sex_age %>%
  filter(sex!="all") %>%
  group_by(week_number, week_number_run_over, year, date_w_c, age, place_of_death) %>%
  summarise(across(starts_with("deaths_"), ~sum(.x)), .groups = "drop")
```

#### Fixing overall weekly deaths

As of 28 July 2021, the [ScotGov open data website](https://statistics.gov.scot/data/deaths-involving-coronavirus-covid-19) reports the non-covid deaths as if they were all-cause deaths, and this messed up my entire pipeline! Below is a quick fix, where instead of using the open data API, I construct the 'overall' weekly deaths data, `weekly_deaths_overall`, from the NRS-published ad-hoc query data on deaths by sex.

```{r}
## quick fix whilst the open data for overall deaths doesn't work...
# TODO: remove this chunk once no longer needed


# weekly_deaths_overall_averages_and_other_bits <-  # keep the nrs-computed past averages from the broken data, as well as other columns needed
#   weekly_deaths_overall %>%
#   select(-c(deaths_covid_related, deaths_non_covid, deaths_all_causes))
#   # select(age, sex, place_of_death, week_number_run_over, deaths_nrs_past_average_all_causes, ref_area, ref_period)
# 
# weekly_deaths_overall <-  # this rewrites weekly_deaths_overall!
#   weekly_deaths_sex %>%
#   group_by_all() %>%
#   ungroup(sex, starts_with("deaths_")) %>%
#   summarise(across(starts_with("deaths_"), .fns = sum), .groups = "drop") %>%  # adds together figures between F & M
#   ({function (x) bind_rows(  # this takes the data at this point and a copy of it, and then summarises the copy across place of death, creating an 'All' category, the figures for which are sums across place of death
#     x,
#     x %>% group_by_all() %>% ungroup(place_of_death, starts_with("deaths_")) %>% summarise(across(starts_with("deaths_"), .fns = sum), .groups = "drop") %>% mutate(place_of_death = "All")
#   )}) %>%
#   mutate(sex = "all", age = "all") %>%  # add these so that I don't end up breaking any further analysis
#   arrange(week_number_run_over, place_of_death) %>%
#   left_join(
#     weekly_deaths_overall_averages_and_other_bits,
#     by = c("week_number", "week_number_run_over", "year", "date_w_c", "place_of_death", "sex", "age")
#     )  # add the average numbers in also so I don't break any further analysis
```


#### Weekly Covid as contributory vs underlying cause of death

```{r}
weekly_covid_contributory_vs_underlying_cause <-
  readxl::read_excel(files_weekly_deaths$sex_age, sheet = 2, skip = 4, col_names = c("week_number","place_of_death","sex","age","cause_of_death","number_of_deaths"), guess_max = 1e5) %>%
  drop_na(number_of_deaths) %>%    # remove empty rows & one row where it's just the copyright notice
  mutate(
    year = 2000L + parse_integer(str_sub(week_number, 1, 2)),
    week_number = parse_integer(str_sub(week_number, 4, 5)),
    date_w_c = compute_start_date_from_week_number(week_number = week_number, year = year),
    week_number_run_over = if_else(year == 2021, 53L + week_number, week_number)  # invent a "run-over" week number - weeks 54+ are in 2021
  ) %>%
  filter(
    cause_of_death %in% c("COVID-19 contributory factor", "COVID-19 underlying cause")
  ) %>%
  tidyr::complete(  # complete implicit missing entries and make them 0
    nesting(week_number,week_number_run_over,date_w_c,year),
    age,sex,place_of_death,cause_of_death,
    fill = list(number_of_deaths = 0)
  ) %>%
  recode_place_of_death() %>%  # this needs to be done after recoding cause of death, otherwise we also include the "all" figure which isn't included in this dataset
  pivot_wider(names_from = cause_of_death, values_from = number_of_deaths) %>%   # make separate columns for causes of death figures
  clean_names() %>%
  filter(age!="all") %>%
  filter(sex!="all") %>%
  filter(place_of_death != "All") %>%
  group_by(week_number, week_number_run_over, year, date_w_c, place_of_death) %>%  # sum over age & sex
  summarise(across(starts_with("covid_"), ~sum(.x)), .groups = "drop") %>%
  ({function(x)
    bind_rows(
      x,
      x %>% 
        group_by(week_number, week_number_run_over, year, date_w_c) %>%  # sum across place of death
        summarise(across(starts_with("covid_"), ~sum(.x)), .groups = "drop") %>%
        mutate(place_of_death = "All")
    )
  })

## check that the figures actually make sense! i.e. the two figures add up to the total covid-related figure
stopifnot(
  (weekly_deaths_overall %>%
  filter(sex=="all" & age=="all") %>%
  inner_join(weekly_covid_contributory_vs_underlying_cause) %>%
  filter(deaths_covid_related != covid_19_contributory_factor + covid_19_underlying_cause) %>%
  nrow) == 0
)

annual_covid_contributory_vs_underlying_cause <-
  weekly_covid_contributory_vs_underlying_cause %>%
  group_by(year, place_of_death) %>%
  summarise(across(starts_with("covid_"), ~sum(.x)), .groups = "drop") %>%
  ({function(x)
    bind_rows(
      x %>% mutate(year = as.character(year)),
      x %>% 
        group_by(place_of_death) %>%  # sum across eyars
        summarise(across(starts_with("covid_"), ~sum(.x)), .groups = "drop") %>%
        mutate(year = "2020-2021")
    )
  }) %>%
  mutate(deaths_covid_related = covid_19_contributory_factor + covid_19_underlying_cause) %>%
  mutate(
    proportion_contributory = covid_19_contributory_factor / deaths_covid_related,
    proportion_underlying = covid_19_underlying_cause / deaths_covid_related
    )
```


### Checking week number coding

```{r}
weekly_deaths_sex_age %>% 
  count(year, date_w_c, week_number, week_number_run_over) %>% 
  slice(1:3, (nrow(.) - 3):nrow(.)) %>% 
  select(-n) %>% 
  knitr::kable(caption = "Checking the computation of dates from week numbers - note how the 1st week of 2020 started in 2019.")
```


## Load historical deaths data

The format of the .xlsx files is that sheet 1 is a table of contents, sheet 2 contains the data, and sheet 3 is a pivot table for subsetting data.

```{r, warning=FALSE}
files_historical_deaths <- set_names(x = as.list(table_of_historical_data_sources$path_latest), nm = table_of_historical_data_sources$short_name)


## overall file - where did I get the link from? it's not obviously available from the website...
# past_deaths_overall_file <-
#   curl_download(url = "https://www.nrscotland.gov.uk/files/statistics/covid19/weekly-deaths-by-location-2015-2019.csv",
#                 destfile = tempfile(),
#                 quiet = FALSE)
# 
# past_deaths_hb_file <-
#   curl_download(url = "https://www.nrscotland.gov.uk/files//statistics/covid19/weekly-deaths-by-date-health-board-location-15-19.xlsx",
#                 destfile = tempfile(),
#                 quiet = FALSE)
# 
# past_deaths_la_file <-
#   curl_download(url = "https://www.nrscotland.gov.uk/files//statistics/covid19/weekly-deaths-by-location-council-areas.xlsx",
#                 destfile = tempfile(),
#                 quiet = FALSE)
# 
# past_deaths_sex_age_file <-
#   curl_download(url = "https://www.nrscotland.gov.uk/files//statistics/covid19/weekly-deaths-by-location-age-group-sex-15-19.xlsx",
#                 destfile = tempfile(),
#                 quiet = FALSE)

past_deaths_overall <- 
  read_csv(files_historical_deaths$overall, skip = 2) %>%
  rename(year = 1) %>%  # rename first column
  mutate(
    place_of_death = if_else(is.na(`1`), true = year, NA_character_)
  ) %>%
  fill(place_of_death, .direction="down") %>%  # LOCF on place_of_death
  drop_na(`1`) %>%
  pivot_longer(cols = all_of(as.character(1:53)), values_to = "number_of_deaths", names_to = "week_number") %>%
  mutate(week_number=as.integer(week_number)) %>%
  recode_place_of_death()

past_deaths_hb <- 
  readxl::read_excel(files_historical_deaths$hb, sheet = 2, skip = 2, col_names = TRUE) %>%  # first two rows are empty, row 3 has headers
  clean_names() %>%
  drop_na(number_of_deaths) %>%  # remove empty rows & one row where it's just the copyright notice
  rename(  # same format as current data
    week_number = matches("week"),
    place_of_death = matches("location"),
    ref_area = health_board
  ) %>%
  mutate(
    week_number = as.integer(week_number)
  ) %>%
  tidyr::complete(
    nesting(week_number,year),  # unique combinations that we don't want to combinatorily explode
    ref_area, place_of_death,  # fill in missing combinations of hb & pod with 0 deaths
    fill = list(number_of_deaths = 0)
  ) %>% 
  recode_place_of_death()  # recode place of death after completing missing combinations, otherwise the factor level "All" gets exploded also

past_deaths_la <-
  readxl::read_excel(files_historical_deaths$la, sheet = 2, skip = 2, col_names = TRUE) %>%  # first two rows are empty, row 3 has headers
  clean_names() %>%
  drop_na(number_of_deaths) %>%  # remove empty rows & one row where it's just the copyright notice
  rename(  # same format as current data
    week_number = matches("week"),
    place_of_death = matches("location"),
    ref_area = council_area
  ) %>%
  mutate(
    week_number = as.integer(week_number)
  ) %>%
  tidyr::complete(
    nesting(week_number,year),  # unique combinations that we don't want to combinatorily explode
    ref_area, place_of_death,  # fill in missing combinations of la & pod with 0 deaths
    fill = list(number_of_deaths = 0)
  ) %>% 
  recode_place_of_death()  # recode place of death after completing missing combinations, otherwise the factor level "All" gets exploded also

past_deaths_sex_age <-  # structure here is a bit more complicated - headers in row 3, but sex & age headers in row 4
  readxl::read_excel(files_historical_deaths$sex_age, sheet = 2, skip = 4, col_names = c("year","week_number","place_of_death","sex","age","number_of_deaths"), guess_max = 1e5) %>%
  drop_na(number_of_deaths) %>%    # remove empty rows & one row where it's just the copyright notice
  mutate(
    year = 2000L + as.integer(year),
    week_number = as.integer(week_number)
  ) %>%
  tidyr::complete(
    nesting(week_number,year),  # unique combinations that we don't want to combinatorily explode
    sex, age, place_of_death,  # fill in missing combinations of age, sex, pod, with 0 deaths
    fill = list(number_of_deaths = 0)
  ) %>% 
  recode_place_of_death()

## compute weekly data for sex and age separately
## sum over sex or age, respectively
past_deaths_sex <-
  past_deaths_sex_age %>%
  group_by_all %>%
  ungroup(age, number_of_deaths) %>%  # this is done by grouping over everything except the number of deaths & sex
  summarise(number_of_deaths = sum(number_of_deaths), .groups = "drop")

past_deaths_age <-
  past_deaths_sex_age %>%
  group_by_all %>%
  ungroup(sex, number_of_deaths) %>%  # this is done by grouping over everything except the number of deaths & sex
  summarise(number_of_deaths = sum(number_of_deaths), .groups = "drop")
```


### Do we get the same number of deaths between datasets?

```{r}
## check that the results are the same as overall deaths
map2_dfr(
  .x = list(past_deaths_overall, past_deaths_hb, past_deaths_hb, past_deaths_sex_age),
  .y = c("overall","health_board","council_area","sex_age"),
  .f = function(data_tbl, name) {
    data_tbl %>%
      mutate(dataset=name,year=as.numeric(year)) %>%
      filter(!is.na(year) & place_of_death!="All") %>%  # to remove summary values from the data
      group_by(dataset,year) %>%
        summarise(total = sum(number_of_deaths), .groups = "drop")
  }
) %>% 
  pivot_wider(names_from=year,values_from=total) %>%
  knitr::kable(caption = "Number of historical deaths by year as calculated from different datasets. The overall dataset and age/sex breakdown agree in figures, and the HB and LA breakdowns agree, but the two pairs differ from each other. The difference is in both directions, some years there are more deaths recorded in one pair, and some years in the other. This may be due to the following (verbatim from data description online): Health Board and Local Authority figures include non-residents. Deaths are allocated to areas based on the usual residence of the deceased. If the deceased was not a Scottish resident, the death is allocated to the area where the death occurred.")
```


## Load map data with `sf`

I previously downloaded shapefiles for:

* Local Authority Districts (LAD) for 2020 (BUC-level resolution): https://geoportal.statistics.gov.uk/datasets/local-authority-districts-december-2020-uk-buc/
* Health Boards https://data.gov.uk/dataset/27d0fe5f-79bb-4116-aec9-a8e565ff756a/nhs-health-boards
  - Note: this is a fairly high resolution, don't need this much detail!
  - I found a discussion of this on [a github covid-related project](https://github.com/tomwhite/covid-19-uk-data/issues/18#issuecomment-622353841), and user **robchallen** produced a zip file containing [UK-wide health board boundaries](https://github.com/tomwhite/covid-19-uk-data/files/4563933/UK_covid_reporting_regions.zip)


```{r}
# shapefile_hb <- st_read(dsn = "./map_data/hb")  # too detailed!
shapefile_hb_uk_wide <- st_read(dsn = "./map_data/hb_uk_wide/UK_covid_reporting_regions")
shapefile_la <- st_read(dsn = "./map_data/la")

# merged_deaths_la %>%
#   filter(week_number==53 & place_of_death=="Home & other non-institution") %>%
#   left_join(shapefile_la,by=c("ref_area"="lad19nm")) %>%
#   ggplot(aes(fill = deaths_all_causes, geometry = geometry, group=ref_area)) +
#   geom_sf()
# 
# merged_deaths_hb %>%
#   filter(week_number==53 & place_of_death=="Home & other non-institution") %>%
#   left_join(shapefile_hb_uk_wide,by=c("ref_area"="name")) %>%
#   ggplot(aes(fill = deaths_all_causes, geometry = geometry)) +
#   geom_sf() +
#   theme(aspect.ratio = 1.3)

# ggplot(data = shapefile_hb_uk_wide, aes(geometry=geometry)) + geom_sf()
  
## simple UK map
# ggplot(data = map_data(map = "world", region = "UK"), aes(x=long, y = lat, group = group)) +
#   geom_polygon() +
#   coord_map()
```


## Load COVID case counts by LA/HB

```{r}
files_case_counts <- set_names(x = as.list(table_of_case_trends_data_sources$path_latest), nm = table_of_case_trends_data_sources$short_name)

cases_hb <- read_csv(
  files_case_counts$hb, 
  col_types = 
    cols(
    .default = col_double(),
    `_id` = col_skip(),
    DailyDeaths = col_skip(),  # not needed, we get this elsewhere!
    Date = col_date(format = "%Y%m%d"),
    HB = col_character(),
    HBName = col_character(),
    HospitalAdmissionsQF = col_character(),
    ICUAdmissionsQF = col_character()
    ),
  ) %>% clean_names() %>%
  rename(ref_area = hb_name) %>%
  mutate(
    year = parse_integer(str_sub(ISOweek(date), start = 1, end = 4)),  # parse ISO year, otherwise the first few days of 2021 will belong to 2021 erroneously!
    week_number = parse_integer(str_sub(ISOweek(date), start = 7, end = 8)),
    ref_area = phsmethods::match_area(hb)  # convert to standard HB names
    )

cases_la <- read_csv(
  file = files_case_counts$la, 
  col_types = 
    cols(
    .default = col_double(),
    `_id` = col_skip(),
    DailyDeaths = col_skip(),  # not needed, we get this elsewhere!
    Date = col_date(format = "%Y%m%d"),
    CA = col_character(),
    CAName = col_character()
  )
) %>% clean_names() %>%
  rename(ref_area = ca_name) %>%
  mutate(
    year = parse_integer(str_sub(ISOweek(date), start = 1, end = 4)),  # parse ISO year, otherwise the first few days of 2021 will belong to 2021 erroneously!
    week_number = parse_integer(str_sub(ISOweek(date), start = 7, end = 8)),
    ref_area = phsmethods::match_area(ca)  # convert to standard LA names, jsut in case they weren't already!
    )
```


# Calculate case counts: weekly counts to match weekly death reporting

Note: the cumulative case counts have already been calculated, we just need to subset them to match the weeks. This is not trivial - the death counts cover an entire week starting from the "w/c date"; the cumulative cases for that week could be taken either at the start, the end of the week, or the start of the following week even!

However, when calculating weekly positive cases we can simply take the maximum of that week's cumulative count - that will be the cumulative count at the end of the week!

Note also that the `crude_rate_positive_per100k` represents the cumulative rate!

```{r}
weekly_covid_cases_hb <-
  cases_hb %>%
  group_by(year, week_number, ref_area) %>%
  summarise(
    weekly_positive = sum(daily_positive),
    cumulative_positive = max(cumulative_positive),
    crude_rate_positive_per100k = max(crude_rate_positive),
    .groups = "drop"
  ) %>%
  mutate(
    date_w_c = compute_start_date_from_week_number(week_number = week_number, year = year),
    week_number_run_over = if_else(year == 2021, 53L + week_number, week_number)
    )

## extract Scotland-wide data and remove from HB split
weekly_covid_cases_overall <- weekly_covid_cases_hb %>% filter(ref_area=="Scotland")
weekly_covid_cases_hb <- weekly_covid_cases_hb %>% filter(ref_area!="Scotland")

weekly_covid_cases_la <-
  cases_la %>%
  group_by(year, week_number, ref_area) %>%
  summarise(
    weekly_positive = sum(daily_positive),
    cumulative_positive = max(cumulative_positive),
    crude_rate_positive_per100k = max(crude_rate_positive),
    .groups = "drop"
  ) %>%
  mutate(
    date_w_c = compute_start_date_from_week_number(week_number = week_number, year = year),
    week_number_run_over = if_else(year == 2021, 53L + week_number, week_number)
    )
```


# Calculate historical averages, minima & maxima

Due to the counting of weeks, most years have 52 weeks with some years (2015 & 2020) having 53. Therefore we need to decide how to deal with week 53 in 2020 since only 2015 has a corresponding weekly count. The solution used here is to calculate a weekly rate for week 53 from week 53 in 2015, and week 52 in 2016-2019 - this represents the "last week of the year", which includes December 31st.

```{r, warning=FALSE}
map2_dfr(
  .x = list(past_deaths_overall, past_deaths_hb, past_deaths_la, past_deaths_sex_age),
  .y = c("overall","health_board","council_area","sex_age"),
  .f = function(data_tbl, name) {
    data_tbl %>%
      mutate(dataset=name,year=as.numeric(year)) %>%
      filter(year %in% 2016:2019 & week_number==53) %>%
      count(dataset,year)
  }
) %>% 
  pivot_wider(names_from=year,values_from=n) %>%
  knitr::kable(caption = "Number of entries by dataset for week number 53 in years 2016-2019 (which only had 52 weeks). Only the 'overall' historical deaths dataset has week 53 entries for years with 52 weeks, and all the entries are for 0 deaths.")

## helper function that creates week 53 data by taking week 52 in years with no year 53 and relabels it as 53
## CAUTION: only use this for computing averages! otherwise it's making up data that wasnae there
create_week_53_data_for_years_with_52_weeks <- function(data_tbl) {
  real_data <-
    data_tbl %>%
    filter(!(year %in% 2016:2019 & week_number==53))  # remove any entries for week 53 in years with 52 weeks - in some datasets they existed but had 0 listed
  fake_data <-
    data_tbl %>%
    filter(year %in% 2016:2019 & week_number==52) %>%
    mutate(week_number=53)
  return(
    bind_rows(
      real_data,
      fake_data
    )
  )
}

historical_weekly_average_overall <-
  past_deaths_overall %>%
  filter(year!="average") %>%  # remove averages
  group_by(year, week_number) %>%  # remove week 53 if it was 0 for all places  - this means there was no week 53!
  filter(!all(number_of_deaths==0 & week_number==53)) %>%
  ungroup %>%
  create_week_53_data_for_years_with_52_weeks %>%
  ungroup %>%
  group_by(week_number,place_of_death) %>% 
  summarise(
    across(number_of_deaths, .fns = list(mean=mean,min=min,max=max)),
    .groups = "drop"
  ) %>% rename_all(~str_replace_all(.x, pattern = "number_of_deaths", replacement = "deaths"))


week_53_text <- "Due to the way ISO 8601 counts weeks, only 2015 in the 2015-2019 period had 53 weeks; averages for week 53 were computed from figures for week 53 in 2015 and week 52 for 2016-2019."

# TODO: weekly averages may be too detailed for this level od subcategorising!

historical_weekly_average_hb <-
  past_deaths_hb %>%
  create_week_53_data_for_years_with_52_weeks() %>%
  group_by(
    week_number, place_of_death, ref_area
  ) %>%
  summarise(
    deaths_mean = mean(number_of_deaths),
    deaths_min = min(number_of_deaths),
    deaths_max = max(number_of_deaths),
    .groups = "drop"
  )

historical_weekly_average_la <-
  past_deaths_la %>%
  create_week_53_data_for_years_with_52_weeks() %>%
  group_by(
    week_number, place_of_death, ref_area
  ) %>%
  summarise(
    deaths_mean = mean(number_of_deaths),
    deaths_min = min(number_of_deaths),
    deaths_max = max(number_of_deaths),
    .groups = "drop"
  )

historical_weekly_average_sex_age <-
  past_deaths_sex_age %>%
  create_week_53_data_for_years_with_52_weeks() %>%
  group_by(
    week_number, place_of_death, age, sex
  ) %>%
  summarise(
    deaths_mean = mean(number_of_deaths),
    deaths_min = min(number_of_deaths),
    deaths_max = max(number_of_deaths),
    .groups = "drop"
  )

historical_weekly_average_age <-
  past_deaths_age %>%
  create_week_53_data_for_years_with_52_weeks() %>%
  group_by(
    week_number, place_of_death, age
  ) %>%
  summarise(
    deaths_mean = mean(number_of_deaths),
    deaths_min = min(number_of_deaths),
    deaths_max = max(number_of_deaths),
    .groups = "drop"
  )

historical_weekly_average_sex <-
  past_deaths_sex %>%
  create_week_53_data_for_years_with_52_weeks() %>%
  group_by(
    week_number, place_of_death, sex
  ) %>%
  summarise(
    deaths_mean = mean(number_of_deaths),
    deaths_min = min(number_of_deaths),
    deaths_max = max(number_of_deaths),
    .groups = "drop"
  )

compute_historical_annual_average_deaths <- function(data_tbl, ...) {
  grouping_vars <- enquos(...)
  data_tbl %>%
    group_by(year, place_of_death, !!!grouping_vars) %>%
    summarise(  # calculate annual totals first...
    number_of_deaths = sum(number_of_deaths), 
    .groups = "keep"
  ) %>%
  ungroup(year) %>%  # remove year as grouping to average over year now
  summarise(
    annual_deaths_mean = mean(number_of_deaths),
    annual_deaths_min = min(number_of_deaths),
    annual_deaths_max = max(number_of_deaths),
    .groups = "drop"
  )
}


historical_annual_average_overall <-
  past_deaths_overall %>%
  compute_historical_annual_average_deaths()

historical_annual_average_hb <-
  past_deaths_hb %>%
  compute_historical_annual_average_deaths(ref_area)

historical_annual_average_la <-
  past_deaths_la %>%
  compute_historical_annual_average_deaths(ref_area)

historical_annual_average_sex_age <-
  past_deaths_sex_age %>%
  compute_historical_annual_average_deaths(sex, age)

historical_annual_average_sex <-
  past_deaths_sex %>%
  compute_historical_annual_average_deaths(sex)

historical_annual_average_age <-
  past_deaths_age %>%
  compute_historical_annual_average_deaths(age)

historical_annual_average_cause_of_death <-  # this isn't quite the right figure because we don't have the raw data to work from, but weekly averages across 2015-2019
  historical_average_cause_of_death %>%
  group_by(place_of_death, cause_of_death) %>%
  summarise(
    annual_deaths_mean = sum(deaths_mean),
    annual_deaths_min = NA_real_,
    annual_deaths_max = NA_real_,
    .groups = "drop"
  )
```


## Check weekly historic averages against NRS computed data

```{r}
weekly_deaths_overall %>%
  filter(sex=="all" & age=="all") %>%
  left_join(historical_weekly_average_overall) %>%
  select(year, week_number, place_of_death, deaths_nrs_past_average_all_causes, deaths_mean) %>%
  filter(round(deaths_nrs_past_average_all_causes) != round(deaths_mean)) %>%
  knitr::kable(caption = "Subset of weekly deaths where calculated mean deaths 2015-2019 differed between the COVID-19 related deaths file and the weekly deaths file (i.e. all other entries had the same figures!). Week 53 was computed as the average of deaths in week 53 in 2015 and week 52 in 2016-2019. For all the other datasets the mean deaths computed agree!")
```


# Calculate moving averages (rolling mean) over multiple weeks

Once data are broken down by region there are very small figures on a weekly basis, so we can compute a moving average over several weeks instead. This is different to merging figures into 2-week chunks, for example, as information from consecutive weeks will overlap.

The average will be computed to the "left" of the current value (a week's moving average is the average of values at this week + n-1 past weeks, or "right" aligned).

Moving averages for historical data will be computed over the previously computed weekly means & ranges. Computing moving averages over historical ranges produces wider spread data than computing ranges of moving averages (see `order_of_min_max_rolling_mean.R`).


```{r}
weeks_to_average <- 2

## compute moving average over weekly data, using average over k weeks
## using historical data to compute the moving average in the first few weeks of current data
## Note: historical data needs to be the non-summarised (raw) data!
## to compute moving average in first few week(s) of year, we add in the previous years' data and code the weeks so that they retain the right order
## i.e. in 2020 the week_number_run_over starts at 1, and the last week of 2019 is coded as 0, the 2nd last as -1, etc.
## once the moving average has been computed, we remove the past data;
## this approach works for 2021 data seamlessly, using the last weeks of 2020 for the moving average in 2021!
compute_moving_average_for_current_data <- function(current_data, historical_data) {
  bind_rows(
    historical_data %>%  # add historical data and format it so that it matches current data
      filter(year == 2019) %>%  # keep only last year before 2020
      rename(deaths_all_causes = number_of_deaths) %>%  # in historical deaths there is no split between covid/non-covid so we rename to all cause deaths
      mutate(week_number_run_over = -(52-week_number)),
    current_data
  ) %>%
    arrange(week_number_run_over) %>%  # just in case the order got messed up, re-order by week_number_run_over for the computation
    group_by_all() %>% ungroup(matches("^deaths_|week|date|year")) %>%  # clever way of getting grouping by ref_area, sex, age, whatever is available
    mutate(across(
        .cols = matches("^deaths_"), 
        .fns = ~zoo::rollmean(.x, k=weeks_to_average, fill=NA, align="right"), 
        .names = "ma{weeks_to_average}w_{.col}"
        )
    ) %>%
    ungroup %>%
    filter(year > 2019)  # keep only current data
}

rolling_deaths_hb <-
  compute_moving_average_for_current_data(current_data = weekly_deaths_hb, historical_data = past_deaths_hb)

rolling_deaths_la <-
  compute_moving_average_for_current_data(current_data = weekly_deaths_la, historical_data = past_deaths_la)

rolling_deaths_sex_age <-
  compute_moving_average_for_current_data(current_data = weekly_deaths_sex_age, historical_data = past_deaths_sex_age)

rolling_deaths_age <-
  compute_moving_average_for_current_data(current_data = weekly_deaths_age, historical_data = past_deaths_age)

rolling_deaths_sex <-
  compute_moving_average_for_current_data(current_data = weekly_deaths_sex, historical_data = past_deaths_sex)



## inspect the calculations  
rolling_deaths_la %>% filter(week_number_run_over %in% c(1:3,52:54) & str_detect(ref_area,"Glasgow") & place_of_death=="Hospital") %>% select(year, week_number_run_over, ref_area, place_of_death, deaths_all_causes, ma2w_deaths_all_causes) %>% knitr::kable(caption="Showing how rolling average was computed: week 1 is average of week 52 of 2019 (not shown) and week 1 of 2020; week 2 is average of weeks 1 & 2, etc.")

rolling_deaths_sex_age %>% filter(week_number_run_over %in% c(1:3,52:54) & str_detect(sex,"F") & age == "65-74" & place_of_death=="Hospital") %>% select(year, week_number_run_over, sex, age, place_of_death, deaths_all_causes, ma2w_deaths_all_causes) %>% knitr::kable(caption = "Showing how rolling average was computed: week 1 is average of week 52 of 2019 (not shown) and week 1 of 2020; week 2 is average of weeks 1 & 2, etc.")



## helper function that takes historical data and computes a moving average over k weeks
## with the additional trick of using the last week of all the years as the week prior to the first week
## strictly speaking we should be using the last week(s) of 2014 to compute the moving average for the first week(s) in 2015, depending on k; and we shouldn't be using the last week of 2019 in the computation either
## however, the overall effect of this is small since we're averaging over the entire 2015-2019 period
## the inaccuracy of this approach increases with increasing k
## the solution for fixing this is straightforward - just download 2014 data as well, and merge the "current" data and the historical data, and then perform the moving average calculation as usual
compute_moving_average_for_historical_data <- function(historical_data) {
  last_week_in_previous_year_required <- 53 + 2 - weeks_to_average  # this computes the last week needed from the previous year: if averaging over 2 weeks, we need week 3, if over 3 weeks, the lat week needed is 52, etc.
  bind_rows(
    historical_data %>%  # keep last week from all years and convert it to week 0
      filter(week_number %in% last_week_in_previous_year_required:53) %>% 
      mutate(week_number = week_number - 53),  # set week number for data in previous to negative numbers but retaining order 
    historical_data
  ) %>%
  arrange(week_number) %>%  # sort in ascending order of week_number
  group_by_all() %>% ungroup(matches("^deaths_"),week_number) %>%
  mutate(across(
      .cols = matches("^deaths_"), 
      .fns = ~zoo::rollmean(.x, k=weeks_to_average, fill=NA, align="right"), 
      .names = "ma{weeks_to_average}w_{.col}"
      )
  ) %>%
  ungroup %>%
  filter(week_number!=0)
}

historical_weekly_average_hb <- compute_moving_average_for_historical_data(historical_weekly_average_hb)
historical_weekly_average_la <- compute_moving_average_for_historical_data(historical_weekly_average_la)
historical_weekly_average_sex_age <- compute_moving_average_for_historical_data(historical_weekly_average_sex_age)
historical_weekly_average_sex <- compute_moving_average_for_historical_data(historical_weekly_average_sex)
historical_weekly_average_age <- compute_moving_average_for_historical_data(historical_weekly_average_age)
```


# Merge weekly COVID/non-COVID-related deaths in 2020/2021 with historic death rates

```{r}
## compute rolling average
merged_deaths_overall <-
  weekly_deaths_overall %>%
  filter(sex=="all" & age=="all") %>%
  inner_join(historical_weekly_average_overall %>% mutate(sex = "all", age = "all"),  # only merge for aggregated data b/c that's all we have!
            by = c("place_of_death", "week_number", "sex", "age"))

merged_deaths_hb <-
  rolling_deaths_hb %>%  # use data with rolling movingincluded
  inner_join(historical_weekly_average_hb, by = c("week_number","ref_area","place_of_death"))

merged_deaths_la <-
  rolling_deaths_la %>%  # use data with rolling movingincluded
  inner_join(historical_weekly_average_la, by = c("week_number","ref_area","place_of_death"))

merged_deaths_sex_age <-
  rolling_deaths_sex_age %>%  # use data with rolling movingincluded
  inner_join(historical_weekly_average_sex_age, by = c("place_of_death", "week_number", "sex", "age"))

merged_deaths_sex <-
  rolling_deaths_sex %>%
  inner_join(historical_weekly_average_sex, by = c("place_of_death", "week_number", "sex"))

merged_deaths_age <-
  rolling_deaths_age %>%
  inner_join(historical_weekly_average_age, by = c("place_of_death", "week_number", "age"))

merged_deaths_cause_of_death <-
  weekly_deaths_cause_of_death %>%
  inner_join(
    historical_average_cause_of_death  , by = c("place_of_death", "week_number", "cause_of_death")
  )
```


## Weekly deaths including past weekly deaths

For this dataset, we simply `bind_rows` weekly deaths by year and place of death with past weekly deaths by year and location. Note that past data for years with 52 weeks includes an entry of 0 deaths for week 53, so we'll remove them, otherwise the date computation is thrown off!

```{r}
weekly_deaths_current_and_past <-
  bind_rows(
    weekly_deaths_overall %>%
    filter(sex=="all" & age=="all" & place_of_death!="All") %>%  # keep only aggregated values over sex & age
    select(year, week_number, week_number_run_over, place_of_death, matches("deaths"), -deaths_nrs_past_average_all_causes)
    ,
    past_deaths_overall %>%
      rename(deaths_all_causes = number_of_deaths) %>%
      filter(place_of_death!="All" & year!="average") %>%  # remove aggregated entries
      mutate(year = as.integer(year)) %>%
      filter(!(year %in% 2016:2019 & week_number==53))  # remove week 53 entries from years 2016-2019
  ) %>%
  mutate(date_w_c = compute_start_date_from_week_number(week_number = week_number, year_number = year))
```



# Annual deaths for 2020 & 2021

For visualising data using maps, we need to compute data for the entire year. Note: the non-covid and covid-related deaths in 2020 don't add up to all cause deaths due to the covid status only starting in week 12!

```{r}
compute_total_deaths_in_year <- function(data_tbl, ...) {
  grouping_vars <- enquos(...)
  data_tbl %>%
    group_by(year, place_of_death, !!!grouping_vars) %>%
    summarise(across(.cols = c(deaths_all_causes, deaths_covid_related, deaths_non_covid), .fns = ~sum(.x,na.rm=TRUE)), .groups = "drop") %>%  # compute sums of all types of death
    mutate(deaths_all_causes_since_covid = deaths_covid_related + deaths_non_covid)  # compute a smaller number of all-cause deaths since covid started (over 2020, covid- and non-covid-related deaths don't add up to all-cause deaths because pre-covid deaths are included also!)
}

compute_ratio_of_annual_deaths_to_historical <- function(data_tbl) {
  data_tbl %>%
    mutate(
      ratio_annual_deaths_to_historical_mean = deaths_all_causes / annual_deaths_mean,
      ratio_annual_deaths_to_historical_min = deaths_all_causes / annual_deaths_min,
      ratio_annual_deaths_to_historical_max = deaths_all_causes / annual_deaths_max
    )
}

total_deaths_la <-
  merged_deaths_la %>%
  compute_total_deaths_in_year(ref_area) %>%
  left_join(historical_annual_average_la, by = c("place_of_death", "ref_area")) %>%
  compute_ratio_of_annual_deaths_to_historical

total_deaths_hb <-
  merged_deaths_hb %>%
  compute_total_deaths_in_year(ref_area) %>%
  left_join(historical_annual_average_hb, by = c("place_of_death", "ref_area")) %>%
  compute_ratio_of_annual_deaths_to_historical

total_deaths_sex <-
  merged_deaths_sex %>%
  compute_total_deaths_in_year(sex) %>%
  left_join(historical_annual_average_sex, by = c("place_of_death", "sex")) %>%
  compute_ratio_of_annual_deaths_to_historical

total_deaths_age <-
  merged_deaths_age %>%
  compute_total_deaths_in_year(age) %>%
  left_join(historical_annual_average_age, by = c("place_of_death", "age")) %>%
  compute_ratio_of_annual_deaths_to_historical

total_deaths_overall <-
  merged_deaths_overall %>%
  compute_total_deaths_in_year() %>%
  left_join(historical_annual_average_overall, by = c("place_of_death")) %>%
  compute_ratio_of_annual_deaths_to_historical

## bespoke computation because format is different... deaths split by cause!
total_deaths_cause_of_death <-
  merged_deaths_cause_of_death %>%
  group_by(year, place_of_death, cause_of_death) %>%
  summarise(deaths = sum(number_of_deaths), .groups="drop") %>%
  left_join(historical_annual_average_cause_of_death, by = c("place_of_death", "cause_of_death")) %>%
  mutate(
    ratio_annual_deaths_to_historical_mean = deaths / annual_deaths_mean
  )
```


# Constants about latest data available

```{r}
## constants for use in graphs & annotations
weeks_available_2021 <- max(merged_deaths_age$week_number[merged_deaths_age$year==2021])
most_recent_date_available_2021 <- max(merged_deaths_age$date_w_c)
earliest_date <- min(merged_deaths_age$date_w_c)
```


# Ordering geographical data by population size

The latest population estimates by HB/LA are available [from NRS Scotland here](https://www.opendata.nhs.scot/dataset/population-estimates).

I'm using the estimates for 2020 (based on 2019 data) to compute the relative sizes of health boards!

```{r}
population_estimate_la_2019 <- read_csv(file = "./population_estimates/ca2019_pop_est_29062021.csv") %>%
  select(Year, CA, Sex, AllAges) %>%
  clean_names() %>%
  make_human_readable_ref_area(ref_area = ca) %>%
  filter(year==2020) %>%
  filter(ca != "Scotland") %>%
  filter(sex == "All") %>%
  rename(la = ca)

population_estimate_hb_2019 <- read_csv(file = "./population_estimates/hb2019_pop_est_29062021.csv") %>%
  select(Year, HB, Sex, AllAges) %>%
  clean_names() %>%
  make_human_readable_ref_area(ref_area = hb) %>%
  filter(year==2020) %>%
  filter(hb != "Scotland") %>%
  filter(sex == "All")
```

## Compute order

```{r}
order_la_by_population <-
  population_estimate_la_2019 %>%
  arrange(desc(all_ages)) %>%
  .$la

order_hb_by_population <-
  population_estimate_hb_2019 %>%
  arrange(desc(all_ages)) %>%
  .$hb
```


# Comparisons of annual rates with current rates

```{r}
table_current_deaths_compared_to_past <-
  bind_rows(
    ## all available data
    merged_deaths_overall %>%
      filter(age == "all" &
               sex == "all"
             ) %>% 
      group_by(year, place_of_death, week_number) %>%
      summarise(
        deaths_all_causes = sum(deaths_all_causes),
        # deaths_non_covid = sum(deaths_non_covid, na.rm = TRUE),
        deaths_covid_related = sum(deaths_covid_related, na.rm = TRUE),
        .groups = "drop"
      ) %>% 
      mutate(deaths_non_covid = deaths_all_causes - deaths_covid_related) %>%
      left_join(
        past_deaths_overall %>% filter(year != "average") %>% group_by(week_number, place_of_death) %>% summarise(
                                           average_weekly_deaths = sum(number_of_deaths) / 5,
                                           .groups = "drop"
                                         ),
        by = c("week_number", "place_of_death")
      ) %>% 
      group_by(place_of_death) %>%
      summarise(
        period = glue::glue("Data from week 1, 2020 to week {weeks_available_2021}, 2021"),
        deaths_all_causes = sum(deaths_all_causes),
        deaths_non_covid = sum(deaths_non_covid, na.rm = TRUE),
        deaths_covid_related = sum(deaths_covid_related, na.rm = TRUE),
        mean_annual_deaths_2015_2019 = sum(average_weekly_deaths),
        .groups = "drop"
      )
    ,
    ## 2020 only
    merged_deaths_overall %>%
      filter(age == "all" &
               sex == "all" &
               year == 2020
             ) %>% 
      group_by(place_of_death) %>%
      summarise(
        period = "2020",
        deaths_all_causes = sum(deaths_all_causes),
        deaths_covid_related = sum(deaths_covid_related, na.rm = TRUE),
        .groups = "drop"
      ) %>% 
      mutate(deaths_non_covid = deaths_all_causes - deaths_covid_related) %>%
    left_join(
      historical_annual_average_overall %>% 
        select(place_of_death, mean_annual_deaths_2015_2019 = annual_deaths_mean),
      by = "place_of_death"
    )
    ,
    ## 2020 from week 12 onwards (including week 12) - pandemic part of 2020
    merged_deaths_overall %>%
      filter(
        age == "all" &
          sex == "all" &
          year == 2020 & week_number >= 12
      ) %>%
      group_by(place_of_death) %>%
      summarise(
        period = "2020, week 12 - week 53",
        deaths_all_causes = sum(deaths_all_causes),
        deaths_covid_related = sum(deaths_covid_related, na.rm = TRUE),
        .groups = "drop"
      ) %>% 
    mutate(deaths_non_covid = deaths_all_causes - deaths_covid_related) %>%
    left_join(
      past_deaths_overall %>% 
        filter(week_number >= 12 & year != "average") %>%
        group_by(place_of_death) %>%
        summarise(mean_annual_deaths_2015_2019 = sum(number_of_deaths) / 5),
      by = "place_of_death"
    )
  ) %>%
  mutate(
    ratio_deaths_all_causes_to_historical = deaths_all_causes / mean_annual_deaths_2015_2019,
    ratio_non_covid_deaths_to_historical = deaths_non_covid / mean_annual_deaths_2015_2019,
    proportion_covid_deaths = deaths_covid_related / deaths_all_causes
  )

write_csv(x = table_current_deaths_compared_to_past, file = file.path(dir_outputs, "annual_death_rates_comparison_with_historical_deaths.csv"))

table_current_deaths_compared_to_past %>% knitr::kable(caption = "Comparing annual home death rate during COVID pandemic to historical deaths at home")
```


# Comparing historical rates of home death to total death to 2020/2021

```{r}
proportions_of_historical_deaths_by_place <-
  past_deaths_overall %>% 
  group_by(year, place_of_death) %>% summarise(number_of_deaths = sum(number_of_deaths), .groups="drop") %>%
  filter(place_of_death != "All" & !is.na(as.integer(year))) %>%
  group_by(year) %>%
  mutate(
    total_deaths = sum(number_of_deaths)
  ) %>%
  ungroup %>%
  mutate(proportion_of_total = number_of_deaths / total_deaths, year=as.integer(year))

proportions_of_current_deaths_by_place <-
  total_deaths_overall %>%
  select(year, place_of_death, number_of_deaths = deaths_all_causes) %>%
  filter(place_of_death != "All") %>%
  group_by(year) %>%
  mutate(
    total_deaths = sum(number_of_deaths)
  ) %>%
  ungroup %>%
  mutate(proportion_of_total = number_of_deaths / total_deaths)


merged_proportions_of_deaths_by_place <-
  bind_rows(
    proportions_of_historical_deaths_by_place,
    proportions_of_current_deaths_by_place
  )
```

## Including non-COVID deaths

Note: non-covid deaths are defined as all-cause deaths minus covid-related deaths. Otherwise the number of non-covid deaths only includes non-covid deaths once covid-related deaths started being reported in 2020 (week 12).

```{r}
proportions_of_current_non_covid_deaths_by_place <-
  total_deaths_overall %>%
  mutate(deaths_non_covid = deaths_all_causes - deaths_covid_related) %>%
  select(year, place_of_death, number_of_deaths = deaths_non_covid) %>%
  filter(place_of_death != "All") %>%
  group_by(year) %>%
  mutate(
    total_deaths = sum(number_of_deaths)
  ) %>%
  ungroup %>%
  mutate(proportion_of_total = number_of_deaths / total_deaths)

merged_proportions_of_non_covid_deaths_by_place <-
  bind_rows(
    proportions_of_historical_deaths_by_place,  # all historical deaths are non-covid!
    proportions_of_current_non_covid_deaths_by_place
  )
```

## By geography

```{r}
proportions_of_deaths_by_place_by_geography <-
  bind_rows(
    ## historic deaths
    bind_rows(  # merge HB & LA past deaths, and tag them appropriately
      past_deaths_hb %>% mutate(geography = "hb"),
      past_deaths_la %>% mutate(geography = "la")
    ) %>%
    group_by(year, place_of_death, ref_area, geography) %>% summarise(number_of_deaths = sum(number_of_deaths), .groups="drop") %>%  # compute the total number of yearly deaths by place of death & geography
    filter(place_of_death != "All" & !is.na(as.integer(year))) %>%  # remove blank/irrelevant entries
    group_by(year, ref_area, geography) %>%
    mutate(
      total_deaths = sum(number_of_deaths)  # compute total annual deaths across place of death, but within each geographical area & year
    ) %>%
    ungroup %>%
    mutate(proportion_of_total = number_of_deaths / total_deaths, year=as.integer(year))  # compute proportion of deaths in any palce relative to total annual deaths in geographical area
    ,
    ## current deaths
    bind_rows(  # merge HB & LA past deaths, and tag them appropriately
      total_deaths_hb %>% mutate(geography = "hb"),
      total_deaths_la %>% mutate(geography = "la")
    ) %>%
    select(year, place_of_death, number_of_deaths = deaths_all_causes, geography, ref_area) %>%  # keep only relevant columns & rename deaths due to all causes
    group_by(year, place_of_death, ref_area, geography) %>% summarise(number_of_deaths = sum(number_of_deaths), .groups="drop") %>%  # compute the total number of yearly deaths by place of death & geography
    filter(place_of_death != "All" & !is.na(as.integer(year))) %>%  # remove blank/irrelevant entries
    group_by(year, ref_area, geography) %>%
    mutate(
      total_deaths = sum(number_of_deaths)  # compute total annual deaths across place of death, but within each geographical area & year
    ) %>%
    ungroup %>%
    mutate(proportion_of_total = number_of_deaths / total_deaths, year=as.integer(year))  # compute proportion of deaths in any palce relative to total annual deaths in geographical area
  )

## write to csv
proportions_of_deaths_by_place_by_geography %>%
  write_csv(file = paste0(dir_outputs,"/proportions_of_deaths_by_place_by_geography.csv"))
```

## Number & proportion of deaths in `Other` location

```{r}
annual_deaths_coded_as_other_place_hb <-
  proportions_of_deaths_by_place_by_geography %>% 
  filter(geography=="hb" & place_of_death == "Other") %>% 
  mutate(num_prop = paste0(format(number_of_deaths, big.mark = ","), " (", scales::percent(proportion_of_total, accuracy = 0.01),")")) %>%
  select(year, place_of_death, ref_area, num_prop) %>% 
  pivot_wider(names_from = year, values_from = num_prop)

annual_deaths_coded_as_other_place_hb %>% write_csv(file = paste0(dir_outputs,"/annual_deaths_coded_as_other_place_hb.csv"))
```


# Excess deaths

## Excess deaths by place of death

```{r}
total_deaths_overall %>%
  mutate(excess_deaths = deaths_all_causes - annual_deaths_min)

excess_deaths_2020_table <-
  past_deaths_overall %>%
  group_by(year, place_of_death) %>%
  summarise(annual_deaths = sum(number_of_deaths), .groups = "drop") %>%
  filter(year==2019) %>%
  pivot_wider(names_from = year, names_prefix = "annual_deaths_", values_from = annual_deaths) %>%
  left_join(
    total_deaths_overall %>% filter(year==2020), by = "place_of_death"
    ) %>%
  mutate(
    excess_deaths_vs_2019 = deaths_all_causes - annual_deaths_2019,
    # excess_deaths_vs_2019_prop = excess_deaths_vs_2019 / sum(excess_deaths_vs_2019),
    excess_deaths_vs_mean = deaths_all_causes - annual_deaths_mean,
    excess_deaths_vs_min = deaths_all_causes - annual_deaths_min,
    excess_deaths_vs_max = deaths_all_causes - annual_deaths_max
  ) %>%
  select(year, place_of_death, annual_deaths_2019, annual_deaths_mean, deaths_all_causes, matches("excess")) %>%
  mutate(disaggregated_place_of_death = place_of_death != "All") %>%
  group_by(disaggregated_place_of_death) %>%
  mutate(
    proportion_excess_deaths_vs_2019 = excess_deaths_vs_2019 / sum(excess_deaths_vs_2019),
    proportion_excess_deaths_vs_mean = excess_deaths_vs_mean / sum(excess_deaths_vs_mean)
  ) %>%
  ungroup %>%
  select(-disaggregated_place_of_death)

excess_deaths_table_2019 <-
  excess_deaths_2020_table %>%
  select(place_of_death, annual_deaths_2019, deaths_all_causes, excess_deaths_vs_2019, proportion_excess_deaths_vs_2019) %>%
  mutate(
    proportion_excess_deaths_vs_2019 = scales::percent(proportion_excess_deaths_vs_2019, accuracy = 0.1)
    ) %>%
  mutate_if(is.numeric, ~format(.x, big.mark=",")) %>%
  rename(
    "Place of death" = place_of_death,
    "2019 deaths" = annual_deaths_2019,
    "2020 deaths (all causes)" = deaths_all_causes,
    "Excess deaths" = excess_deaths_vs_2019,
    "Proportion of excess deaths by place" = proportion_excess_deaths_vs_2019
  )

write.xlsx(x = list("Excess w.r.t. 2019" = excess_deaths_table_2019), file = paste0(dir_outputs,"/excess_deaths_Scotland_compared_to_2019.xlsx"))
```


# Deaths over time


## Number of deaths by year

```{r}
# TODO: finish this here or elsewhere!
# total_deaths_overall
# 
# past_deaths_overall %>%
#   group_by(year, place_of_death) %>%
#   summarise(number_of_deaths = sum(number_of_deaths, na.rm = TRUE), .groups="drop")
# 
# # TODO: can this be generalised in a function?
# merged_deaths_age %>%
#   # filter(place_of_death %in% c("Home & other non-institution")) %>%
#   group_by(year, place_of_death, age, week_number) %>%
#   summarise(
#     deaths_all_causes = sum(deaths_all_causes),
#     deaths_covid_related = sum(deaths_covid_related, na.rm = TRUE),
#     .groups = "drop"
#   ) %>% 
#   mutate(deaths_non_covid = deaths_all_causes - deaths_covid_related) %>%
#   left_join(
#     past_deaths_age %>% filter(year != "average") %>% group_by(week_number, place_of_death, age) %>% summarise(
#                                        average_weekly_deaths = sum(number_of_deaths) / 5,
#                                        .groups = "drop"
#                                      ),
#     by = c("week_number", "place_of_death", "age")
#   ) %>% 
#   group_by(place_of_death, age) %>%
#   summarise(
#     period = glue::glue("Data from week 1, 2020 to week {weeks_available_2021}, 2021"),
#     deaths_all_causes = sum(deaths_all_causes),
#     deaths_non_covid = sum(deaths_non_covid, na.rm = TRUE),
#     deaths_covid_related = sum(deaths_covid_related, na.rm = TRUE),
#     mean_annual_deaths_2015_2019 = sum(average_weekly_deaths),
#     .groups = "drop"
#   ) %>%
#   mutate(
#     percent_increase_deaths_over_historic = (deaths_all_causes / mean_annual_deaths_2015_2019) - 1,
#     percent_increase_non_covid_deaths_over_historic = (deaths_non_covid / mean_annual_deaths_2015_2019) - 1
#     ) %>%
#   (function(x) {
#     bind_rows(
#       x,
#       x %>% group_by(age, period) %>% summarise(
#         deaths_all_causes = sum(deaths_all_causes),
#         deaths_non_covid = sum(deaths_non_covid),
#         mean_annual_deaths_2015_2019 = sum(mean_annual_deaths_2015_2019)
#       ) %>%
#         mutate(
#           percent_increase_deaths_over_historic = (deaths_all_causes / mean_annual_deaths_2015_2019) - 1,
#           percent_increase_non_covid_deaths_over_historic = (deaths_non_covid / mean_annual_deaths_2015_2019) - 1
#         )
#     )
#   }) %>%
#   View()
#   # kable(caption = "Total deaths at home in data so far for age groups.")
```


# Save most recent workspace for further analyses

```{r}
# save.image(file = "./workspace.RData", safe = TRUE)  # this is calling .GlobalEnv, which is empty
save(list = ls(all.names = TRUE), file = "./workspace.RData", envir = environment())
```

# Print session info

```{r}
sessionInfo()
```

